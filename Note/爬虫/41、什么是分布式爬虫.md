##### 什么是分布式爬虫

在多台机器上执行同一个爬虫程序，爬取网站数据



##### `scrapy`能否实现

不能。调度器、管道文件不能在多台机器上共享，导致不能多台机器一起根据同一网站调度要下载的网页，也不能将数据持久化到同一个文件中



##### 如何实现分布式爬虫

通过`scrapy-redis`组件



##### 下载安装`scrapy-redis`

`pip3 install scrapy-redis`



##### 修改`Redis.conf`配置文件

##### 1、注释`bind 127.0.0.1`

允许除本机以外的机器访问

##### 2、将`protected-mode` 改为`no`，关闭保护模式

当其他计算机连接后，允许操作数据库中的数据



##### 基于`CrawlSpider`创建爬虫文件：

**知识点1：**基于`from scrapy_redis.spiders import RedisCrawlSpider`引入继承的父类

**知识点2：**不使用`start_urls`列表，而是使用`redis_key`变量

```python
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

from scrapy_redis.spiders import RedisCrawlSpider

class QiubaiSpider(RedisCrawlSpider):
    name = 'qiubai'

    redis_key = 'qiubai_key'

    link = LinkExtractor(allow=r'Items/')

    rules = (
        Rule(link, callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        pass
```

