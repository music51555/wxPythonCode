##### 抓取指定页面时使用

##### 基于`scrapy.Spider`实现抓取`start_urls`中页面数据的爬虫程序：

1、创建项目`scrapy startprocet wangyiPro`

2、创建爬虫应用`scrapy genspider baidu www.baidu.com`

3、通过`scrapy genspider baidu www.baidu.com`创建爬虫程序

4、在`parse`方法中得到`start_urls`网页的响应对象`response`，并通过`xpath`方法解析页面数据

5、在`items.py`中定义变量

6、引入`items`中的类进行实例化，封装变量

7、在爬虫程序的最后`yield item`

8、在项目的`settings.py`中开启`pipeline`文件

9、在项目的`pipeline`中，解析`item`对象得到变量，并进行持久化存储



#####　抓取页面的二级页面时使用

##### 基于`scrapy.Spider`实现发起`get`和`post`请求的爬虫程序

1、创建项目`scrapy startprocet wangyiPro`

2、创建爬虫应用`scrapy genspider baidu www.baidu.com`

3、通过`yield scrapy.Request(url = xxx, callback= xxx，meta={'item:item'})`实现发起`get`请求，指定`callback`函数解析页面数据，并传递`item`对象

4、通过`yield scrapy.FormRequest(url = xxx, formdata = { }, callback = xxx)`发起`post`请求

5、通过重写`scrapy.Spider`父类中的`start_requests`方法，定义`post`请求内容的字典后，循环`start_urls`来发起`post`请求

```python
def start_requests(self):
    data = {
        'kw': 'dog'
    }

    for url in self.start_urls:
        yield scrapy.FormRequest(url = url, formdata = data, callback=self.parse)
```





##### 基于`CrawSpider`实现通过`link`和`rule`按正则匹配多页链接的爬虫程序：

1、创建项目`scrapy startprocet wangyiPro`

2、创建爬虫应用`scrapy genspider -t crawl chouti dig.chouti.com`

3、创建爬虫程序`scrapy genspider -t crawl baidu www.baidu.com`

4、定义链接提取器`link`

```python
link = LinkExtractor(allow=r'/all/hot/recent/\d+')
```

5、定义规则提取器`rule`

```python
rules = (
    Rule(link, callback='parse_item', follow=True),
)
```

6、在`parse`方法中解析页面数据



##### 基于`CrawlSpider`和`RedisCrawlSpider`实现的分布式爬虫程序，实现在多台机器部署代码，一同抓取数据，流程几乎一样，区别是有了链接提取器和规则提取器

**分布式爬虫的底层实现逻辑：**开始爬虫后，在`redis`数据库中会创建`"tongcheng:dupefilter"`的去重器，当多台机器执行编写好的爬虫代码时，分别在去重器中验证是否有重复请求的`URL`，对不重复的`URL`进行解析，一定要在每次爬虫前，清除`"tongcheng:dupefilter"`去重器，否则当执行爬虫程序时，会遇到`no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)`错误，没有根据正则表达式，解析出更多的页面请求

1、创建项目`scrapy startprocet wangyiPro`

2、创建爬虫应用

##### `CrawlSpider`：`scrapy genspider -t crawl chouti dig.chouti.com`  

##### `RedisSpider`：`scrapy genspider wangyi news.163.com`

3、`pip3 install scrapy_redis`

4、引入并继承父类

##### `CrawlSpider`：`from scrapy_redis.spiders import RedisCrawlSpider`

##### `RedisSpider`：`from scrapy_redis.spiders import RedisSpider`

5、使用`redis_key`代替`start_urls`，用于在`redis`数据库中存放爬取到的数据

```python
redis_key = 'wangyi'
```

##### `redis.conf`配置区：

6、修改`Redis.conf`配置文件，注释`bind 127.0.0.1`

7、修改`Redis.conf`配置文件，将`protected-mode` 改为`no`，关闭保护模式

8、修改`Redis.conf`配置文件，配置在后台启动`daemonize`改为`yes`

```shell
# 注释bind 127.0.0.1，允许其他计算机通过IP访问
bind 127.0.0.1
# 取消保护模式，允许其他计算机修改数据库数据
protected-mode yes
# 使用后台的方式启动redis-server
daemonize yes
```



##### 项目的`settings.py`配置区：

9、在`settings.py`中配置`redis`数据库连接信息，`HOST`和`PORT`和`REDIS_PARAMS = {'password':'123456'}`

```python
REDIS_HOST = '192.168.3.82'
REDIS_PORT = 6379
REDIS_PARAMS = {'password':'123456'}
```

10、在`settings.py`中配置使用`redis`的调度器`SCHEDULER`、去重器`Filter`

```python
SCHEDULER = 'scrapy_redis.scheduler.Scheduler'
```

11、在`settings.py`中配置使用`redis`的`pipeline`管道文件

```python
DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'
```

12、是否允许暂停爬虫程序

```python
SCHEDULER_PERSIST = True
```

13、定义使用Redis的pipeline管道存储数据

```python
ITEM_PIPELINES = {
   'scrapy_redis.pipelines.RedisPipeline': 300,
}
```



##### `CrawlSpider`的独有代码：

13、`link`链接提取器，通过正则表达式匹配网页中的`URL`

14、`rule`规则提取器，对匹配到的`URL`发起请求，执行指定函数，以及通过`follow`参数设置是否应用于新网页



15、在`parse_item`方法中解析响应对象中的网站数据

16、在函数的结尾`yield item`，此时是将`item`返回到`redis`的`pipeline`中

17、启动`redis-server`和`redis-cli`

18、通过`scrapy runspider xxx.py`运行爬虫程序，程序开始监听`redis`数据库

```shell
#启动程序后，程序开始监听，等待在redis中输入网址
DevTools listening on ws://127.0.0.1:12945/devtools/browser/3b939670-ddd3-408b-8f33-af9543fee473
```



19、在`redis`中`lpush redis_key 爬取的网址`

20、程序开始爬取，并在`lrange redis_key 0 -1`中查询爬取到的数据













































