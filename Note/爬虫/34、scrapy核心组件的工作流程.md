`scrapy`核心组件：

首先`scrapy`**引擎**会先判断**爬虫程序**中是否有`start_url`，如果有则依次发起请求，所有的请求被存储在**调度器**中，调度器会调用**下载器**请求下载页面，将下载的响应对象返回给爬虫程序，爬虫程序通过响应对象获取页面数据，并将页面数据封装在`item`对象中，通过`yield`关键字返回给`pipeline`文件，并在其中做持久化存储操作



**引擎：**触发事务，调用`start`

**调度器：**接收引擎发送的请求，排入请求队列中，同时进行`url`的去重操作

**下载器：**下载网页内容，并将内容返回给爬虫程序

**爬虫程序：**调用`parse`方法执行数据解析

**管道：**执行持久化存储