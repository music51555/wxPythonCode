爬虫知识点总结：

1、什么是爬虫？

```
编写代码模拟浏览器浏览网页，抓取数据
```

2、什么是`robots.txt`协议

```
网站规定了哪些数据可以爬取，哪些数据不可以爬取，访问`https://www.taobao.com/robots.txt
```

3、什么是反爬机制

```
发起请求后，判断请求头的`User-Agent`的载体信息，判断用户时通过浏览器还是通过爬虫程序提交的请求
```

4、什么是反反爬机制

```python
# 伪装请求头中的`User-Agent`为其他浏览器信息
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',
}
```

5、`requests`模块是做什么用的

```
python原生的基于网络请求的模块，模拟浏览器发起请求
```

6、如何安装`requests`模块

```
`pip3 install requests`
```

7、如何通过`requests`模块发起`get`请求

```python
import requests

url = 'https://www.baidu.com'
# 支持带有参数的url
url = 'https://www.sogou.com/web?query=可口可乐&ie=utf8'

params = {
    'query': '周杰伦',
    'ie': 'urf8'
}

# 可以将get请求的参数写在url中，但是更简明的操作是将参数封装在params参数中，有s
response = requests.get(url = url, params = params)
```

8、如何获取`get`请求得到的网页内容

```python
with open('baidu.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
```

9、发起请求得到的`response`有哪些方法

```python
response.content
# 1、得到网页的二进制bytes类型数据
b'<!DOCTYPE html>\r\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css><title>\xe7\x99\xbe\xe5\xba\xa6\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe4\xbd\xa0\xe5\xb0\xb1\xe7\...

response.status_code
# 2、请求状态码
200

response.headers
# 3、获取响应头信息
{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'Keep-Alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Sat, 29 Dec 2018 06:54:51 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:23:56 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}

response.url
# 4、获取请求的url
https://www.baidu.com/
```

10、`get`请求如何封装请求头信息

```PYTHON
# 复制UA后，封装headers字典，添加headers参数，有s
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

response = requests.get(url = url, params = params, headers = headers).text
```

11、`requests`模块如何发起`post`请求

```python
import requests

url = 'https://www.douban.com/accounts/login'

data = {
    'source': 'index_nav',
    'form_email': '18611848257',
    'form_password': 'nishi458_2',
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

# post请求的参数封装在data参数中，get请求封装在params参数中
response = requests.post(url = url, data = data, headers = headers)

with open('./douban.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
```

12、`requests`如何发起`ajax`的`get`和`post`请求

```python
# 方式和get、post请求是一致的
requests.get()
requests.post(0)
```

13、获取到`ajax`的请求的`json`数据后，如何处理

```python
# 调用requests.get().josn()的json方法，最后强制转换为list数据类型后打印结果
response = list(requests.get(url = url, params = params, headers = headers).json())

print(response.text)
```

14、如何发起ajax的post请求

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

response = requests.post(url = url, params = params,headers = headers)
```

