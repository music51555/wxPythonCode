爬虫知识点总结：

1、什么是爬虫？

```
编写代码模拟浏览器浏览网页，抓取数据
```

2、什么是`robots.txt`协议

```
网站规定了哪些路径可以爬取，哪些数据不可以爬取，访问https://www.taobao.com/robots.txt

User-agent:  Baiduspider
Allow:  /article
Allow:  /oshtml
Allow:  /ershou
Allow: /$
Disallow:  /product/
Disallow:  /

User-Agent:  Googlebot
Allow:  /article
Allow:  /oshtml
Allow:  /product
Allow:  /spu
Allow:  /dianpu
Allow:  /oversea
Allow:  /list
Allow:  /ershou
Allow: /$
Disallow:  /
```

3、什么是反爬机制

```
发起请求后，判断请求头的`User-Agent`的载体信息，判断用户时通过浏览器还是通过爬虫程序提交的请求
```

4、什么是反反爬机制

```python
# 伪装请求头中的`User-Agent`为其他浏览器信息
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',
}
```

5、`requests`模块是做什么用的

```
python原生的基于网络请求的模块，模拟浏览器发起请求
```

6、如何安装`requests`模块

```
pip3 install requests
```

7、如何通过`requests`模块发起`get`请求

```python
import requests

url = 'https://www.baidu.com'
# 支持带有参数的url
url = 'https://www.sogou.com/web?query=可口可乐&ie=utf8'

params = {
    'query': '周杰伦',
    'ie': 'urf8'
}

# 可以将get请求的参数写在url中，但是更简明的操作是将参数封装在params参数中，有s
response = requests.get(url = url, params = params)
```

8、如何获取`get`请求得到的网页内容

```python
with open('baidu.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
```

9、发起请求得到的`response`有哪些方法

```python
response.content
# 1、得到网页的二进制bytes类型数据
b'<!DOCTYPE html>\r\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css><title>\xe7\x99\xbe\xe5\xba\xa6\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe4\xbd\xa0\xe5\xb0\xb1\xe7\...

response.status_code
# 2、请求状态码
200

response.headers
# 3、获取响应头信息
{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'Keep-Alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Sat, 29 Dec 2018 06:54:51 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:23:56 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}

response.url
# 4、获取请求的url
https://www.baidu.com/
```

10、`get`请求如何封装请求头信息

```PYTHON
# 复制UA后，封装headers字典，添加headers参数，有s
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

response = requests.get(url = url, params = params, headers = headers).text
```

11、`requests`模块如何发起`post`请求

```python
import requests

url = 'https://www.douban.com/accounts/login'

data = {
    'source': 'index_nav',
    'form_email': '18611848257',
    'form_password': 'nishi458_2',
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

# post请求的参数封装在data参数中，get请求封装在params参数中
response = requests.post(url = url, data = data, headers = headers)

with open('./douban.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
```

12、`requests`如何发起`ajax`的`get`和`post`请求

```python
# 方式和get、post请求是一致的
requests.get()
requests.post(0)
```

13、获取到`ajax`的请求的`json`数据后，如何处理

```python
# 调用requests.get().josn()方法，最后强制转换为list数据类型后打印结果
response = list(requests.get(url = url, params = params, headers = headers).json())

print(response.text)
```

14、如何发起`ajax`的`post`请求

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

response = requests.post(url = url, params = params,headers = headers)
```

15、如何抓取多页的数据

```python
# 循环页数
for page in range(start_page, end_page):
	params = {
        'query': key_word,
        'page': page,
        'ie': 'urf8'
	}
	
	# 每一次请求的参数不同，传入params中变量
	requests.get(url=url, params=params, headers = headers)
```

16、如何继承上一页面的cookie信息

```python
# 创建session对象，通过session对象发起get或post请求
session = requests.session()

# 通过session对象发起get和post请求
session.get()

session.post()
```

17、什么是代理？

```
使用代理IP访问目标网页，防止因访问次数过多而限制IP
```

18、如何封装代理信息

```python
proxy = {
    # 协议要和目标IP的协议一致
    'http':'112.217.199.122:55872'
}
```

19、如何在请求中使用代理信息

```python
requests.get(url = url, proxies = proxy, params = params, headers = headers)
```

20、从哪引入`etree`类，该类是做什么用的

```python
# 根据网页内容，创建tree对象，解析网页内容
from lxml import etree
```

21、如何创建`tree`对象，它有什么方法

```python
tree = etree.HTML(response.text)
tree.xpath(...)
```

22、如何通过`tree`对象，解析网页内容

```python
img_url = tree.xpath('//*[@id="captcha_image"]/@src')[0]
```

23、云打码平台使用什么版本的

```
PythonHTTP
```

24、如何使用云打码平台

```python
from yundama.YDM3 import *

def get_valid_code(file_name):
    # 用户名
    username = 'music51555'

    # 密码
    password = 'nishi458_2'

    # 软件ID
    appid = 6535

    # 软件密钥
    appkey = '0d9170626841d7fea4f1a0674f240616'

    # 传入的图片路径
    filename = file_name

    # 验证码识别类型 http://www.yundama.com/price.html
    codetype = 3000

    # 超时时间，秒
    timeout = 20

    # 检查
    if (username == 'username'):
        print('请设置好相关参数再测试')
    else:
        # 初始化
        yundama = YDMHttp(username, password, appid, appkey)

        # 登陆云打码
        uid = yundama.login();
        print('uid: %s' % uid)

        # 查询余额
        balance = yundama.balance();
        print('balance: %s' % balance)

        # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果
        cid, result = yundama.decode(filename, codetype, timeout);
        print('cid: %s, result: %s' % (cid, result))

        return result
```

25、`re.findall()`的结果是什么类型的

```
列表
```

26、如何在正则表达式中取消贪婪模式

```python
import re
# (.*?)中的?表示取消贪婪模式，因为会匹配双引号中间的许多内容，添加?后，表示只匹配第一个
img_list = re.findall('<div class="thumb">.*?<img src="(.*?)"', pic_text,re.S)
```

27、如何使用`rsplit`方法，并只取第一个结果

```python
str.rspilt('/',1)[1]
```

28、`tree.xpath()`中如何按属性查找标签

```python
tree.xpath('//div[@class = "name"]')
```

29、`xpath`中如何找到第二个索引位置的标签

```python
tree.xpath('//div[@class = "name"]/p[2]')
```

30、`xpath`中如何使用逻辑运算符

```python
tree.xpath('//a[@class = "name" and @href = "hello"]')
```

31、`xpath`中如何进行模糊匹配，查找`href`属性包含`llo`的标签

```python
tree.xpath("//a[contains(@class,'llo')]")
```

32、`xpath`中如何查找以`he`开头的标签

```python
tree.xpath('//a[starts-with(@href,"he")]')
```

33、如何获取某个标签下的文本内容

```python
tree.xpath('//div[@class = "name"]/text()')
```

34、`xpath`如何获取所有子标签的文本

```python
tree.xpath('//div[@class = "name"]//text()')
```

35、`xpath`如何获取某个标签的属性值

```python
tree.xpath('//div[@class = "name"]/@href')
```



36、通过`xpath`方法得到结果是什么类型，有什么特点

```
得到的结果是Element类型，可以继续调用xpath方法
```

37、如何通过`xpath`方法获取一组数据

```python
# 得到的结果是列表，每一个元素的类型都是Element
list_li = etree.xpath('//li[@class="list_li"]')
```

38、`xpath`中的`点`表示什么意思

```python
# 表示在当前标签下继续查找
content = li.xpath('./div[@class="content"]/text()')[0]
```

39、在使用`BeautifulSoup`模块之前需要安装哪个依赖模块

```
pip3 install lxml
```

40、如何创建`BeautifulSoup`对象

```python
soup = BeautifulSoup(response.text,'lxml')
```







































