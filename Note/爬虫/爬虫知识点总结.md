爬虫知识点总结：

1、什么是爬虫？

```
编写代码模拟浏览器浏览网页，抓取数据
```

2、什么是`robots.txt`协议

```python
# 网站规定了哪些路径可以爬取，哪些路径不可以爬取，访问https://www.taobao.com/robots.txt

User-agent:  Baiduspider
Allow:  /article
Allow:  /oshtml
Allow:  /ershou
Allow: /$
Disallow:  /product/
Disallow:  /

User-Agent:  Googlebot
Allow:  /article
Allow:  /oshtml
Allow:  /product
Allow:  /spu
Allow:  /dianpu
Allow:  /oversea
Allow:  /list
Allow:  /ershou
Allow: /$
Disallow:  /
```

3、反爬机制有哪些

```
1、判断请求头的`User-Agent`的载体信息，判断用户时通过浏览器还是通过爬虫程序提交的请求
2、robots协议，允许用户可以抓取数据的路径
3、短时间连接次数过多，封IP
4、登录时添加验证码机制
5、数据动态加载
6、数据加密
```

4、什么是反反爬机制

```python
# 伪装请求头中的`User-Agent`为其他浏览器信息
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',
}
```

5、`requests`模块是做什么用的

```
python原生的基于网络请求的模块，模拟浏览器发起请求
```

6、如何安装`requests`模块

```
pip3 install requests
```

7、如何通过`requests`模块发起`get`请求

```python
import requests

url = 'https://www.baidu.com'
# 支持带有参数的url
url = 'https://www.sogou.com/web?query=可口可乐&ie=utf8'

params = {
    'query': '周杰伦',
    'ie': 'urf8'
}

# 可以将get请求的参数写在url中，但是更简明的操作是将参数封装在params参数中，有s
response = requests.get(url = url, params = params)
```

8、如何获取`get`请求得到的网页内容

```python
with open('baidu.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
```

9、发起请求得到的`response`有哪些方法

```python
response.content
# 1、得到网页的二进制bytes类型数据
b'<!DOCTYPE html>\r\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css><title>\xe7\x99\xbe\xe5\xba\xa6\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe4\xbd\xa0\xe5\xb0\xb1\xe7\...

response.status_code
# 2、请求状态码
200

response.headers
# 3、获取响应头信息
{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'Keep-Alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Sat, 29 Dec 2018 06:54:51 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:23:56 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}

response.url
# 4、获取请求的url
https://www.baidu.com/
```

10、`get`请求如何封装请求头信息

```PYTHON
# 复制UA后，封装headers字典，添加headers参数，有s
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

response = requests.get(url = url, params = params, headers = headers).text
```

11、`requests`模块如何发起`post`请求

```python
import requests

url = 'https://www.douban.com/accounts/login'

data = {
    'source': 'index_nav',
    'form_email': '18611848257',
    'form_password': 'nishi458_2',
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

# post请求的参数封装在data参数中，get请求封装在params参数中
response = requests.post(url = url, data = data, headers = headers)

with open('./douban.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
```

12、`requests`如何发起`ajax`的`get`和`post`请求

```python
# 方式和get、post请求是一致的
requests.get()
requests.post(0)
```

13、获取到`ajax`的请求的`json`数据后，如何处理

```python
# 调用requests.get().josn()方法，最后强制转换为list数据类型后打印结果
response = list(requests.get(url = url, params = params, headers = headers).json())

print(response.text)
```

14、如何发起`ajax`的`post`请求

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}

response = requests.post(url = url, params = params,headers = headers)
```

15、如何抓取多页的数据

```python
# 循环页数
for page in range(start_page, end_page):
	params = {
        'query': key_word,
        'page': page,
        'ie': 'urf8'
	}
	
	# 每一次请求的参数不同，传入params中变量
	requests.get(url=url, params=params, headers = headers)
```

16、如何继承上一页面的cookie信息

```python
# 创建session对象，通过session对象发起get或post请求
session = requests.session()

# 通过session对象发起get和post请求
session.get()

session.post()
```

17、什么是代理？

```
使用代理IP访问目标网页，防止因访问次数过多而限制IP
```

18、如何封装代理信息

```python
proxy = {
    # 协议要和目标IP的协议一致
    'http':'112.217.199.122:55872'
}
```

19、如何在请求中使用代理信息

```python
requests.get(url = url, proxies = proxy, params = params, headers = headers)
```

20、从哪引入`etree`类，该类是做什么用的

```python
# 根据网页内容，创建tree对象，解析网页内容
from lxml import etree
```

21、如何创建`tree`对象，它有什么方法

```python
tree = etree.HTML(response.text)
tree.xpath(...)
```

22、如何通过`tree`对象，解析网页内容

```python
img_url = tree.xpath('//*[@id="captcha_image"]/@src')[0]
```

23、云打码平台使用什么版本的

```
PythonHTTP
```

24、如何使用云打码平台

```python
from yundama.YDM3 import *

def get_valid_code(file_name):
    # 用户名
    username = 'music51555'

    # 密码
    password = 'nishi458_2'

    # 软件ID
    appid = 6535

    # 软件密钥
    appkey = '0d9170626841d7fea4f1a0674f240616'

    # 传入的图片路径
    filename = file_name

    # 验证码识别类型 http://www.yundama.com/price.html
    codetype = 3000

    # 超时时间，秒
    timeout = 20

    # 检查
    if (username == 'username'):
        print('请设置好相关参数再测试')
    else:
        # 初始化
        yundama = YDMHttp(username, password, appid, appkey)

        # 登陆云打码
        uid = yundama.login();
        print('uid: %s' % uid)

        # 查询余额
        balance = yundama.balance();
        print('balance: %s' % balance)

        # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果
        cid, result = yundama.decode(filename, codetype, timeout);
        print('cid: %s, result: %s' % (cid, result))

        return result
```

25、`re.findall()`的结果是什么类型的

```
列表
```

26、如何在正则表达式中取消贪婪模式

```python
import re
# (.*?)中的?表示取消贪婪模式，因为会匹配双引号中间的许多内容，添加?后，表示只匹配第一个
img_list = re.findall('<div class="thumb">.*?<img src="(.*?)"', pic_text,re.S)
```

27、如何使用`rsplit`方法，并只取第一个结果

```python
str.rspilt('/',1)[1]
```

28、`tree.xpath()`中如何按属性查找标签

```python
tree.xpath('//div[@class = "name"]')
```

29、`xpath`中如何找到第二个索引位置的标签

```python
tree.xpath('//div[@class = "name"]/p[2]')
```

30、`xpath`中如何使用逻辑运算符

```python
tree.xpath('//a[@class = "name" and @href = "hello"]')
```

31、`xpath`中如何进行模糊匹配，查找`href`属性包含`llo`的标签

```python
tree.xpath("//a[contains(@class,'llo')]")
```

32、`xpath`中如何查找以`he`开头的标签

```python
tree.xpath('//a[starts-with(@href,"he")]')
```

33、如何获取某个标签下的文本内容

```python
tree.xpath('//div[@class = "name"]/text()')
```

34、`xpath`如何获取所有子标签的文本

```python
tree.xpath('//div[@class = "name"]//text()')
```

35、`xpath`如何获取某个标签的属性值

```python
tree.xpath('//div[@class = "name"]/@href')
```

36、通过`xpath`方法得到结果是什么类型，有什么特点

```
得到的结果是Element类型，可以继续调用xpath方法
```

37、如何通过`xpath`方法获取一组数据

```python
# 得到的结果是列表，每一个元素的类型都是Element
list_li = etree.xpath('//li[@class="list_li"]')
```

38、`xpath`中的`点`表示什么意思

```python
# 表示在当前标签下继续查找
content = li.xpath('./div[@class="content"]/text()')[0]
```

39、在使用`BeautifulSoup`模块之前需要安装哪个依赖模块

```
pip3 install lxml
```

40、如何创建`BeautifulSoup`对象

```python
# 网络请求的内容
soup = BeautifulSoup(response.text,'lxml')

# 或文件对象
f = open('xxx','wb')
BeautifulSoup(f,'lxml')
```

41、`soup`对象如何查找到第一个标签对象

```python
# 查找到第一个div标签
soup.div

# 查找到第一个div标签，输出标签内容
<div class="wrap"><div id="header"><div id="top"><spa...</div>
```

42、`soup`对象如何输出标签属性

```python
print(soup.div.attrs)
{'class': ['wrap']}

print(soup.div['class'][0])
wrap
```

43、`soup`对象如何输出标签下的文本内容

```python
# 通过string、text、get_text()方法
soup.a.string
soup.a.text
soup.a.get_text()

经典美文
```

44、`soup`对象如何通过`find`方法查找标签

```python
soup.find('div')

# 查找到第一个div标签，输出标签内容
<div class="wrap"><div id="header"><div id="top"><spa...</div>
```

45、`soup`对象如何通过`find`方法和标签属性查找标签

```python
# find(标签名称，属性key=value)
print(soup.find('div',class_='wrap'))
```

46、`soup`对象如何通过`findAll`方法查找到所有标签内容，结果是什么类型的

```python
soup.findAll('div')

[<div class="wrap"><div id="h...]
```

47、`soup`对象如何通过`findAll`方法查找所有标签的中的前2个

```python
# 通过limit属性
soup.findAll('a',limit=2)

[<a href="http://meiwen.ishuo.cn/" target="_blank">经典美文</a>, <a href="http://www.ishuo.cn/" target="_blank">经典语录网</a>]
```

48、`soup`对象如何通过`findAll`方法查找到所有的a标签和`div`标签

```python
soup.findAll(['a','div'])
```

49、`soup`对象如何通过选择器查找标签，结果是什么类型

```python
soup.select('#top')
# 结果是列表类型
[<div id="top"><span class="right"><a href=...]

# 类选择器
soup.select('.right')

# 后代选择器
soup.select('#top .right')

# 属性选择器
soup.select('soup.select('a[href="/member/4"]')')

# 层级选择器
soup.select('#hd_logo > a')
```

50、通过`soup.select()`得到的列表，循环后，其元素内容是否继续支持`bs4`方法

```python
# 支持
title_list = soup.select('.book-mulu a')

# 打开文件存储每一篇文章的标题+正文
with open('shuihu.txt', 'w', encoding='utf-8') as f:
    for title_tag in title_list:
    	pass
```

51、如何安装`selenium`模块

```python
pip3 install selenium
```

52、在哪下载selenium的驱动和查询和浏览器的映射关系

```
http://chromedriver.storage.googleapis.com/index.html

https://www.cnblogs.com/JHblogs/p/7699951.html
```

53、从哪引入`webdriver`类

```
from selenium import webdriver
```

54、`selenium`如何创建`Chrome`浏览器的`driver`对象

```python
driver = webdriver.Chrome('....exe')
```

55、`driver`对象如何打开网页

```python
driver.get('xxxxx')
```

56、无界面的浏览器`phantomjs`驱动下载

```
http://phantomjs.org/download.html
```

57、如何创建`Phantomjs`的`driver`对象

```python
driver = webdriver.PhantomJS('.../bin/phantomjs')
```

58、`driver`对象如何保存浏览器截图

```python
driver.save_screenshot('...png')
```

59、`driver`对象如何执行`js`脚本

```python
driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')
```

60、`js`如何滚动页面到底部

```javascript
window.scrollTo(0,document.body.scrollHeight)
```

61、`driver`对象如何获取页面源码

```python
page_text = driver.page_source
```

62、爬虫中的`scrapy`是什么

```
pass
```

63、`linux`环境下如何安装`scrapy`

```
pip3 install scrapy
```

64、`scrapy`是如何工作的

```
首先scrapy引擎会先判断爬虫程序中是否有start_url，如果有则依次发起请求，所有的请求被存储在调度器中，调度器会在去重URL后调用下载器请求下载页面，将下载的响应对象返回给爬虫程序，爬虫程序通过响应对象获取页面数据，并将页面数据封装在item对象中，通过yield关键字返回给pipeline文件，并在其中做持久化存储操作
```

64、`windows`环境下如何安装`scrapy`

```
1、pip3 install wheel，和egg一样，是一种打包格式

2、下载Twisted,Twisted是用python实现的基于事件驱动的网络引擎框架 

首先通过python3 -V命令查询出python版本后选择对应的内容下载

https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted

pip3 install 下载的框架.whl

3、pip3 install pywin32，python可以直接操控win32程序 

4、pip3 install scrapy，为了爬取网站数据而编写的应用框架

5、终端输入scrapy测试
```

65、如何通过`scrapy`框架创建`scrapy`项目，他的目录结构时什么样的

```python
scrapy startproject myscrapy

- projectname
	- projectname
		- spiders # 在该目录下编写爬虫文件
			- __init__.py
		- __init__.py
		- items.py # 数据结构化：将在爬虫文件中解析的数据进行数据结构化
		- middlewares.py # 中间件
		- pipelines.py # 管道文件：持久化存储处理
		- settings.py # 配置文件：递归层数、并发数、延迟下载等
	- scrapy.cfg # 项目主配置文件：一般不进行修改
```

66、`scrapy`如何创建爬虫应用

```shell
cd myscrapy

# 下载后将在spider文件夹下将创建qiubai.py爬虫程序
scrapy genspider qiubai www.qiushibaike.com
```

67、默认创建的爬虫程序是什么样的

```python
# -*- coding: utf-8 -*-
import scrapy

class DoubanSpider(scrapy.Spider):
    name = 'douban'
    # allowed_domains = ['www.douban.com'] 注释该句，可以爬取该网站域名以外的图片
    start_urls = ['http://www.douban.com/']

    def parse(self, response):
        pass
```

68、创建的爬虫程序，必须继承于哪个类，从哪引入

```python
import scrapy

scrapy.Spider
```

69、在爬虫程序`parse`方法中如何获取页面内容

```python
def parse(self, response):
    # 通过response.text方法
	print(response.text)
```

70、使用`scrapy`框架时需要做哪些配置

```python
# 是否遵从网站的ROBOTS协议，如https://www.baidu.com/robots.txt，默认为True，改为False
ROBOTSTXT_OBEY = True

# 修改USER_AGENT
USER_AGENT = 'user-agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'
```

71、如何开始爬行`crawl`爬虫程序，是否需要进入应用目录下执行

```shell
# 当有多个爬虫应用时，进入到爬虫应用的目录下执行
scrapy crawl qiubai --nolog
```

72、爬虫应用中`response`提供了什么方法，它的结果类型是什么样的，元素类型是什么样的

```python
response.xpath('...')

# 结果类型时列表，元素类型是selector，可以继续使用xpath方法
```

73、如何提取`xpath`方法得到的结果

```python
# xpath得到的结果，需要通过extract()[0]或extract_first()方法获取数据
author = qiushi_item.xpath('.//div[@class="author clearfix"]//h2/text()').extract_first()
```

74、`xpath`得到的结果是一个列表，循环该列表，继续使用`xpath`查找，应如何编写

```python
# 添加.，表示在当前结果下继续查找
.//div[@class="author clearfix"]//h2/text()
```

75、`scrapy`支持哪些持久化存储

```python
# 1、通过命令导出为csv文件
# 2、通过打开文件的方式存储
# 3、通过pipeline管道文件存储
```

76、如何想进行数据持久化存储为`csv`文件，在`parse`方法中如何返回结果

```python
# 如果想进行数据持久化存储，必须返回一个可迭代对象或空
def parse(self, response):
    # 每一次循环将结果封装在一个字典中，再将字典添加到列表中，返回一个可迭代的对象
    duanzi_info = {
        'author': author,
        'content': content
    }

    data_list.append(duanzi_info)
    
	return data_list
```

77、如何通过命令将爬取到的内容存储为`csv`文件

```python
# 添加-o参数，设置导出文件名
scrapy crawl qiubai -o qiubai.csv --nolog

# 如果爬取的内容为乱码，则需要在配置文件中配置
FEED_EXPORT_ENCODING = 'gbk'
```

78、在进行管道存储时，`item.py`文件的作用是

```python
# 存储页面解析的数据
import scrapy

class MyscrapyItem(scrapy.Item):
    author = scrapy.Field()
    content = scrapy.Field()
```

79、如何将变量提交给`item`对象

```python
from .. import items

item = items.MyscrapyItem()

item['author'] = author
item['content'] = content

yield item
```

80、如何在`pipeline.py`中将抓取的内容保存

```python
# 通过函数的item参数，获取item对象中保存的变量
class MyscrapyPipeline(object):
    def process_item(self, item, spider):
        author = item['author']
        content = item['content']

        with open('qiushi.txt', 'a', encoding='utf-8') as f:
            f.write(author + '\n' + content + '\n\n')
```

81、使用`pipeline`保存文件前，需要做什么配置

```python
# 在settings.py中开启配置项
ITEM_PIPELINES = {
   'qiubai_text.pipelines.QiubaiTextPipeline': 300,
}
```

82、在`pipeline.py`中保存文件时，`w`模式只会每次覆盖已保存的内容，怎样避免该现象

```python
# 1、使用a模式
with open('qiushi.txt', 'a', encoding='utf-8') as f:
    f.write(author + '\n' + content + '\n\n')

# 2、在pipeline.py中使用open_spider()方法和close_spider()方法
class MyscrapyPipeline(object):
    def open_spider(self, spider):
        self.f =  open('qiushi.txt', 'a', encoding='utf-8')

    def process_item(self, item, spider):
        author = item['author']
        content = item['content']

        self.f.write(author + '\n' + content + '\n\n')

    def close_spider(self, spider):
        self.f.close()
```

83、在`soup.select()`中如果要使用`nth-child(2)`要写成什么样

```python
nth-of-type(2)
```

84、如何在`pipeline.py`中使用`pymysql`模块

```python
import pymysql

class MyscrapyPipeline(object):
    def open_spider(self, spider):
        self.conn = pymysql.connect(
            host = '140.143.132.118',
            port = 3306,
            user = 'xiaoxin',
            password = 'Nishi458_2',
            db = 'qiubai',
            charset = 'utf8'
        )

    def process_item(self, item, spider):
        author = item['author']
        content = item['content']

        sql = 'insert into qiushi(author, content) values(%s, %s)'

        self.cursor = self.conn.cursor()
        self.cursor.execute(sql,(author,content))
        self.conn.commit()

    def close_spider(self, spider):
        self.conn.close()
        self.cursor.close()
```

85、`pymysql`模块中`conn`捕获到异常时如何进行回滚操作

```python
try:
    self.cursor = self.conn.cursor()
    self.cursor.execute(sql,(author,content))
    1asdas
    self.conn.commit()
except Exception:
    self.conn.rollback()
```

86、如何安装`redis`模块

```
pip3 install redis
```

87、如何创建`redis`的`conn`数据库连接对象

```python
import redis

self.conn = redis.Redis(host='127.0.0.1', port=6379, charset='utf8')
```

88、如何将数据存储到`redis`数据库

```python
# 将数据存储到字典中，通过lpush方法存储到redis数据库
def process_item(self, item, spider):
        author = item['author']
        content = item['content']

        dict = {
            'author': author,
            'content': content
        }

        self.conn.lpush('qiubai_data', str(dict))
```

89、如何在`redis`数据库中查询出数据

```python
# 通过lrange方法
lrange qiubai_key 0 -1
```

90、如果在`pipeline.py`中有多个类，分别将数据存储到文件、`mysql`和`redis`，需要注意什么

```
1、在每一个类中的def process_item(self, item, spider)中return item，否则下一个类中的item的类型变为NoneType
2、在settings.py中开启配置项，设置优先级，依次执行
```

91、如果通过`soup.select`选择器获取到的列表中，如何得到最后一个元素

```
通过列表[-1]
```

92、在`scrapy`框架中如何发起`get`请求，并执行回调函数

```python
yield scrapy.Request(url = url, callback = self.parse)
```

93、在`scrapy`框架中如何发起`post`请求，并执行回调函数

```python
yield scrapy.FormRequest(url = url, formdata = data, callback = self.parse)
```

94、`scrapy`中的访问每一个页面后的`cookie`信息时如何存储的

```
依次访问页面后，cookie信息时自动携带的
```

95、`scrapy`框架在哪设置代理信息

```
在moddleware.py中
```

96、在`middleware.py`文件中如何设置代理信息

```python
def process_request(self, request, spider):
    request.meta['proxy'] = 'http://176.215.254.44:60899'

    return None

# 并在settings.py中开启DOWNLOADER_MIDDLEWARES设置
DOWNLOADER_MIDDLEWARES = {
   'scrapy_pro.middlewares.ScrapyProDownloaderMiddleware': 543,
}
```

97、日志等级分为哪几种

```
INFO：一般
WARNING：警告
ERROR：错误
DEBUG：调试
```

98、如何设置`scrapy`的日志等级

```python
# 在settings.py中设置
LOG_LEVEL = 'ERROR'
```

99、如何设置将`scrapy`日志保存到文件

```python
# 在settings.py中设置
LOG_FILE = 'log.log'
```

100、浏览二级页面详情页时，`scrapy`发起了`get`请求，如何传递`item`参数

```python
# 通过meta参数传递
yield scrapy.Request(url = url, callback = self.parse, meta = {'item':item})
```

101、传递`item`参数后，如何获取`item`

```python
item = response.meta['item']
```

101、爬虫应用中有多个函数解析页面数据，在哪`yield item`

```
在最后顺序的函数中
```

102、什么是`CrawlSpider`

```
CrawlSpider是Spider的一个子类，包含链接提取器、规则解析器，当要提取多页内容时使用
```

103、如何创建基于`CrawlSpider`的爬虫应用

```shell
# 与创建spider的爬虫应用相比，多了一个crawl -t
scrapy genspider (-t crawl) appname www.baidu.com
```

104、基于`crawlspider`创建的爬虫应用有哪2个特点

```python
# 链接提取器
link = LinkExtractor(allow=r'Items/')

# 规则提取器
rules = (
    Rule(link, callback='parse_item', follow=True),
)
```

105、默认创建基于`crawlspider`创建的爬虫时什么样的

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

class ChoutiSpider(CrawlSpider):
    name = 'chouti'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/']

    link = LinkExtractor(allow=r'Items/')

    rules = (
        Rule(link, callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        i = {}
        return i
```

106、如何使用基于`crawlspider`创建的爬虫应用

```pythoN
# 通过链接提取器在页面中匹配页数，传入规则提取器
link = LinkExtractor(allow=r'/all/hot/recent/\d+')

# 规则提取器对所有匹配到的连接发起请求，并调用回调函数parse_item解析数据
 rules = (
     Rule(link, callback='parse_item', follow=True),
 )
```

107、规则提取器中的`follow`参数表示什么意思

```
表示在新的页面中是否继续应用规则提取器
```

108、如何通过实例爬取每一页帖子的标题

```python
class ChoutiSpider(CrawlSpider):
    name = 'chouti'
    # allowed_domains = ['www.baidu.com']
    start_urls = ['https://dig.chouti.com/']

    link = LinkExtractor(allow=r'/all/hot/recent/\d+')

    rules = (
        Rule(link, callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        title = response.xpath('//a[@class="show-content color-chag"]/text()').extract_first().strip()
        print(title)
```

109、什么是分布式爬虫

```
在多台机器上一起执行爬虫应用，抓取数据
```























